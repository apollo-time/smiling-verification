{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (28709, 48, 48) (28709,)\n",
      "Valid set (3589, 48, 48) (3589,)\n",
      "Test set (3589, 48, 48) (3589,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'fer2013.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "    \n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Valid set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (28709, 48, 48, 1) (28709, 7)\n",
      "Validation set (3589, 48, 48, 1) (3589, 7)\n",
      "Test set (3589, 48, 48, 1) (3589, 7)\n"
     ]
    }
   ],
   "source": [
    "image_size = 48\n",
    "num_labels = 7\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "    \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "kernel_size = 5\n",
    "c3_kernel = 4\n",
    "c1_depth = 64\n",
    "c2_depth = 96\n",
    "c3_depth = 128\n",
    "c4_depth = 256\n",
    "fc1_nodes = 2048\n",
    "fc2_nodes = 2048\n",
    "\n",
    "beta = 0.001\n",
    "starter_learning_rate = 0.0001\n",
    "\n",
    "alexnet_simple = tf.Graph()\n",
    "\n",
    "with alexnet_simple.as_default():\n",
    "    #Input data\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32, \n",
    "        shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32,\n",
    "        shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    \n",
    "      # Variables.\n",
    "    c1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [kernel_size, kernel_size, num_channels, c1_depth], stddev=0.1))\n",
    "    c1_bias = tf.Variable(tf.zeros([c1_depth]))\n",
    "\n",
    "    c2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [kernel_size, kernel_size, c1_depth, c2_depth], stddev=0.1))\n",
    "    c2_bias = tf.Variable(tf.zeros([c2_depth]))\n",
    "    \n",
    "    c3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [c3_kernel, c3_kernel, c2_depth, c3_depth], stddev=0.1))\n",
    "    c3_bias = tf.Variable(tf.zeros([c3_depth]))\n",
    "    \n",
    "    c4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [c3_kernel, c3_kernel, c3_depth, c4_depth], stddev=0.1))\n",
    "    c4_bias = tf.Variable(tf.zeros([c4_depth]))\n",
    "    \n",
    "    fc1_weights = tf.Variable(tf.truncated_normal(\n",
    "                [image_size //4 * image_size //4 * c4_depth, fc1_nodes], stddev=0.1))\n",
    "    fc1_bias = tf.Variable(tf.zeros([fc1_nodes]))\n",
    "    \n",
    "    fc2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [fc1_nodes, fc2_nodes], stddev=0.1))\n",
    "    fc2_bias = tf.Variable(tf.zeros([fc2_nodes]))\n",
    "                          \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal(\n",
    "            [fc1_nodes, num_labels], stddev=0.1))\n",
    "    hidden_bias = tf.Variable(tf.zeros([num_labels]))\n",
    "                    \n",
    "   \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "                          \n",
    "    def model(data, drop=False):\n",
    "        conv = tf.nn.conv2d(data, c1_weights, [1,1,1,1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + c1_bias)\n",
    "        pooled = tf.nn.max_pool(hidden, ksize=[1,3,3,1],\n",
    "                                strides=[1,2,2,1], padding='SAME')\n",
    "        #pooled_norm = tf.nn.local_response_normalization(pooled)\n",
    "                          \n",
    "        conv = tf.nn.conv2d(pooled, c2_weights, [1,1,1,1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + c2_bias)\n",
    "        pooled = tf.nn.max_pool(hidden, ksize=[1,3,3,1],\n",
    "                                strides=[1,2,2,1], padding='SAME')\n",
    "        #pooled_norm = tf.nn.local_response_normalization(pooled)\n",
    "        \n",
    "        conv = tf.nn.conv2d(pooled, c3_weights, [1,1,1,1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + c3_bias)\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, c4_weights, [1,1,1,1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + c4_bias)\n",
    "        \n",
    "        #hidden_norm = tf.nn.local_response_normalization(hidden)\n",
    "        \n",
    "        shape = hidden.get_shape().as_list()\n",
    "                          \n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        if drop:\n",
    "            reshape_drop = tf.nn.dropout(reshape, keep_prob=keep_prob)\n",
    "\n",
    "            fc1 = tf.nn.relu(tf.matmul(reshape_drop, fc1_weights) + fc1_bias)\n",
    "            fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "            \n",
    "            #fc2 = tf.nn.relu(tf.matmul(fc1_drop, fc2_weights) + fc2_bias)\n",
    "            #fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob)\n",
    "            return tf.matmul(fc1_drop, hidden_weights) + hidden_bias\n",
    "            \n",
    "        else:\n",
    "            fc1 = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_bias)\n",
    "            #fc2 = tf.nn.relu(tf.matmul(fc1, fc2_weights) + fc2_bias)\n",
    "            return tf.matmul(fc1, hidden_weights) + hidden_bias           \n",
    "    \n",
    "    logits = model(tf_train_dataset, False)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    l2_loss = beta * ( tf.nn.l2_loss(c1_weights) + tf.nn.l2_loss(c1_bias) +\n",
    "                   tf.nn.l2_loss(c2_weights) + tf.nn.l2_loss(c2_bias) +\n",
    "                   tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_bias) +\n",
    "                   tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_bias) +\n",
    "                   tf.nn.l2_loss(hidden_weights) + tf.nn.l2_loss(hidden_bias) +\n",
    "                   tf.nn.l2_loss(c3_weights) + tf.nn.l2_loss(c3_bias)  +\n",
    "                   tf.nn.l2_loss(c4_weights) + tf.nn.l2_loss(c4_bias)  \n",
    "                 )\n",
    "\n",
    "    # global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                           1000, 0.96, staircase=True)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(starter_learning_rate).minimize(loss+l2_loss)\n",
    "                          \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    valid_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(valid_prediction, tf_valid_labels))\n",
    "    \n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "    summary = tf.scalar_summary(\"training_error\", loss)\n",
    "    validation_summary = tf.scalar_summary(\"test_error\", valid_loss)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Restore from:  ./checkpoints/more_convolution_l2_1e3_r1e4/more_convolution_l2_1e3_r1e4-100000\n",
      "Minibatch loss at step 100000: 0.001110\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 50.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 140001\n",
    "model_name = 'more_convolution_l2_1e3_r1e4'\n",
    "train_writer = tf.train.SummaryWriter('./summary/'+ model_name)\n",
    "#valid_writer = tf.train.SummaryWriter('./summary/valid')\n",
    "\n",
    "\n",
    "with tf.Session(graph=alexnet_simple) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  initial_step = 0\n",
    "\n",
    "  ## Load saved checkpoint\n",
    "  ckpt = tf.train.get_checkpoint_state(\n",
    "    os.path.dirname('./checkpoints/' + model_name + '/checkpoint'))\n",
    "\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    # Restore from checkpoint\n",
    "    saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    initial_step = int(ckpt.model_checkpoint_path.rsplit('-', 1)[1])\n",
    "    print(\"Restore from: \", ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    try:\n",
    "        os.mkdir('./checkpoints/' + model_name)\n",
    "    except:\n",
    "        print(\"It's ok\")\n",
    "    \n",
    "  for step in range(initial_step, num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "    _, l, predictions, = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step > 0 and step % 100 == 0):\n",
    "      train_summ, valid_summ = session.run([summary, validation_summary], feed_dict=feed_dict)\n",
    "      train_writer.add_summary(valid_summ, step)\n",
    "      train_writer.add_summary(train_summ, step)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "    \n",
    "      # print('Learning rate: %f' % (rate))\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "        \n",
    "      #valid_predict = session.run([valid_prediction], feed_dict=feed_dict)\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    if (step % 1000 == 0):\n",
    "        saver.save(session, './checkpoints/' + model_name + '/' + model_name, global_step=step)\n",
    "  #test_predict = session.run([test_prediction], feed_dict=feed_dict)\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  saver.save(session, './checkpoints/' + model_name, global_step=num_steps)\n",
    "  session.close()\n",
    "\n",
    "\n",
    "train_writer.close()\n",
    "\n",
    "#valid_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
